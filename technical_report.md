The goal of this project is to create a model which can flag specific drugs newly released in the FDA’s database which have a higher potential of having a Class I or Class II recall by the FDA within 2 years or their labeling.  This could be very beneficial to doctors and patients to know if the new drugs they are using or considering using will have a higher risk of being recalled.  It could also be beneficial to pharmaceutical companies and investors to test their drugs on this model to see if the model things this specific drug has a higher risk of being recalled, before they invest time and money into the drug. 

The FDA has open-source data on all drugs that are labeled by the FDA at this website:  https://open.fda.gov/downloads/.  I downloaded the 6 files of drug labels, which are in json format, converted them to csv format, and then combined them into one large csv file.  I also did some basic cleaning of the data, including extracting the release date and year of each drug to be used for classification, as well as each drug’s NDC (National Drug Code) number to uniquely identify each drug.  This number was recorded in a paragraph of text, which had to be extracted from the text, and then stored in its own column.

I then gathered all the Class I and Class II recalls from the FDA’s enforcement report at this website: https://www.accessdata.fda.gov/scripts/ires/index.cfm#tabNav_advancedSearch.  For this dataset as well, I extracted the date of the enforcement and the NDC number found in a text paragraph for each drug that was recalled.  Finally, I joined the 2 datasets on the unique NDC identifier.  If the recall date of the drug was within 2 years of the release date, I coded the target column as a 1, and if there was no recall on that specific drug, or if there was a recall but it occurred after 2 years of the release, I coded the target column as a 0.  Since there are about 80,000 drugs in the dataset, and less than 0.5% of which were recalled, I needed to find the optimal way to split my data set so there would be recalled drugs both in my training data as well as in my testing set.  The optimal split was to predict on recalls within 2 years, which would leave all data from 2015 to present to be my testing set, with several drugs recalled already, but not yet 2 years of time elapsed to let it be used to train my model. Because the rate at which new drugs are being released is growing rapidly each year, my split divided the data evenly with about 40,000 drugs released from 2010-2014, and another 40,000 from 2015-present.  When fitting my model, I did a RandomSearch cross-validation to ensure my model was not over fit, but besides that I felt it wasn’t practical to do any other test-train-split so as not to further bifurcate my small amount of target values.

Looking at the 80 text columns of data for each of the drugs, I noticed that many columns were empty for many drugs.  I assigned a cutoff, that if 70% of the drugs had no information in a specific column, I turned that column into a simple Boolean column, encoded as a 1 for “containing information” and as a 0 for null.  This took 71 columns and turned them into Booleans, and left 9 columns of text to work with.  For each of these 9 columns I sanitized all the text by removing capital letters, punctuation, stop words, and stemmed each word.  

Using these 9 columns of sanitized text, I set out to perform natural language processing by using a tfidf vectorizer, but my computer couldn’t process such a large data set, so I instead used a count vectorizer to look at the 1000 most common words from each column, and then used a tfidf transformer to look at the unique differences between the rows in each column.  I then used Truncated SVD to perform dimensional reduction of each 1000-column tfidf transformation into only 100 columns.  Joining together the tsvd of each of the original 9 columns, I now had 900 columns to base my model on.
Because less than 0.5% of drugs get recalled, I couldn’t score a model in the standard way, since any classification model would simply predict a 0, or “no recall”, for every drug and would be correct 99.5% of the time.  So, when implementing my grid search I scored my model based on recall score.  The text data was already transformed with NLP, and returned non-explainable numeric values, so I concluded a Random Forest Classifier would be the best model type, and the fact that it operates in a “black box” would not be detrimental to my project, because all its data was already transformed into a black box.  

Looking at all the columns from my 9 original NLP columns together, I was not able to predict a single drug to be recalled.  However, when I fed each of these 9 original columns into my model separately, I detected that that only one of them had predictive power:  the “adverse reactions” column.  Using only this column and the 71 Boolean columns mentioned earlier to fit my model, I could correctly identify 63 drugs in my test set that will be recalled, out of 150 that actually were recalled.  The recall score for this model is 42%, and it had a precision of 2%.  This might not sound like a lot, but for a patient, doctor, or pharmaceutical investor to see a short list of 4,000 out of 40,000 new drugs that are flagged in a “potential recall” category, they might give a little more thought before getting involved with these specific drugs.
