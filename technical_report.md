The goal of this project is to create a model which can flag specific drugs newly released in the FDA’s database which have a higher potential of having a Class I or Class II recall by the FDA within 2 years or their release.  

The FDA has open-source data on all drugs that are labeled (released) by the FDA at this website:  https://open.fda.gov/downloads/.  I downloaded these 6 files of drug labels, which are in json format, converted them to csv format, and then combined them into one large csv file.  I also did some basic cleaning of the data, including extracting the release date and year of each drug to be used for classification, as well as each drug’s NDC (National Drug Code) number to uniquely identify each drug.  This number was recorded in a paragraph of text, which had to be extracted from the text, and then stored in its own column.

I then gathered all the Class I and Class II recalls from the FDA’s enforcement report at this website: https://www.accessdata.fda.gov/scripts/ires/index.cfm#tabNav_advancedSearch.  For this data set as well, I extracted the date of the enforcement and the NDC number found in a text paragraph for each drug that was recalled.  Finally, I joined the 2 datasets on the unique NDC identifier.  If the recall date of the drug was within 2 years of the release date, I coded the target column as a 1, and if there was no recall on that specific drug, or if there was a recall but it occurred after 2 years of the release, I coded the target column as a 0.  Since there were about 80,000 drugs in the data set, and less than 1% of which were recalled, I need to find the optimal way to split my data set so there would be recalled drugs both in my training data as well as in my testing set.  The optimal split was to predict on recalls with 2 years, which would leave all data from 2015 and 2016 to be my testing set, which had several drugs recalled already, but not yet 2 years of time elapsed to let it be used to train my model. Because the rate at which new drugs in being released in growing rapidly each year, my split divided the data evenly with about 40,000 drugs released from 2009-2014, and another 40,000 from 2015-present.  When fitting my model I didn’t a grid search cross-validation to ensure my model is not over fit, but besides that I felt it wasn’t practical to do any other test-train-split so as not to further bifurcate my small amount of target values.

Looking at the 80 text columns of data for each of the drugs, I noticed that many columns were empty for many drugs.  I assigned a cutoff that if 70% of the drugs had no information in a specific column, I turned that column into a simple Boolean column, encoded as a 1 for “containing information” and as a 0 for null.  This took 71 columns and turned them into Booleans, and left 9 columns of text to work with.  For each of these 9 columns I sanitized all the text by removing capital letters, punctuation, stop words, and stemming each word.  

Using these 9 columns of sanitized text, I set out to perform NLP by using a tfidf vectorizer, but my computer couldn’t process such a large data set, so I instead used a count vectorizer to look at the 1000 most common words from each column, and then used a tridf transformer to look at the unique difference between the rows in each column.  I then used Truncated SVD to perform dimensional reduction of each 1000 column tfidf transformation into only 100 columns.  Joining together the tsvd of each of the original 9 columns, I now had 900 columns to base my model on.
Because less than 0.5% of drugs get recalled, I couldn’t score a model in the standard way, because any classification model would simply predict a 0, or “no recall”, for every drug would still be correct 99.5% of the time.  So, when implementing my grid search I scored my model based on recall score.  Since the text data was already transformed with NLP, and returned non-explainable numeric values, I concluded a Random Forest Classifier would be the best model type, and the fact that it operates in a “black box” would not be detrimental to my project, because all its data was already transformed into a black box.  

Looking at all the columns from my 9 original NLP columns, I was not able to predict a single drug to be recalled.  When I fed each of these 9 original columns into my model separately, I detected that 7 of them had no predictive ability, but 2 of them did.  The most predictive column was “adverse reactions”.  Using only this column and the 71 Boolean columns mentioned earlier to fit my model, I could correctly identify 12 drugs in my test set that will be recalled out of 150 that actually were recalled.  The recall score for this model is 8%, and it had a precision of 0.4%.  After doing a grid search cross-validation, and looking at the hyperparameters “criterion”, “n_estimators”, and “min_sample_leaf” in SciKitLearn’s RandomForestClassifier I was able to improve my model to a precision of 0.8%, thereby doubling my precsion, without losing any of my recall.  
